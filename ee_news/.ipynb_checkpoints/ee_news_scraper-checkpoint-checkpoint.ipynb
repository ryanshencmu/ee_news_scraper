{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1254928a-c6f7-47f6-9b3a-6a704f4f3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraper packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#Data Cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime \n",
    "import itertools\n",
    "import re\n",
    "\n",
    "#Data Science\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Text analysis\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.draw.dispersion import dispersion_plot\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis # Visualize the topics\n",
    "import gensim.corpora as corpora# Create Dictionary\n",
    "from pprint import pprint# number of topics\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "#OS Packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle \n",
    "from distutils.version import LooseVersion\n",
    "from collections import Counter\n",
    "\n",
    "#Suppressing warnings over out of date packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Wordcloud\n",
    "from wordcloud import WordCloud# Join the different processed titles together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f157982b-4100-402a-a95c-f7504d1784fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (4.65.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ryanshen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryanshen/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.10.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.8.4)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: gensim in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (68.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim->pyLDAvis)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/ryanshen/Desktop/opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading simpful-2.11.0-py3-none-any.whl (32 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=aecf670ff27dd7c0bcadf0e5bd0c98c0a0df81b20b3baa73c4bbaa0fe8582f2f\n",
      "  Stored in directory: /Users/ryanshen/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=79e963cfad9f830cdcc724e5ce069655777cbad776112ad56e71a9564a147289\n",
      "  Stored in directory: /Users/ryanshen/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: funcy, simpful, miniful, fst-pso, pyfume, FuzzyTM, pyLDAvis\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 funcy-2.0 miniful-0.0.6 pyLDAvis-3.4.1 pyfume-0.2.25 simpful-2.11.0\n"
     ]
    }
   ],
   "source": [
    "#Modules I had to download onto my machine for this project\n",
    "#!pip install textblob\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f579e3-e92b-4179-b395-6cb0426e356c",
   "metadata": {},
   "source": [
    "<h1>House Representative Members: Data Scraping and Cleaning</h1>\n",
    "<h3>As an option we can use this link to get environmental scorecard data #https://scorecard.lcv.org/members-of-congress</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c3592e-7074-495e-80d9-7fad16066c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_house_representatives():\n",
    "    \n",
    "    def custom_date_parser(value): #This function is encapsulated because it's customized for this particular dataframe\n",
    "        try:\n",
    "            return pd.to_datetime(value, format='%Y')\n",
    "        except:\n",
    "            return pd.to_datetime(1900, format='%Y')\n",
    "\n",
    "        driver = webdriver.Firefox() #Ensuring that we move the geckodriver to the user/bin prior to running\n",
    "        driver.get('https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_House_of_Representatives')\n",
    "        house_members = driver.find_element_by_xpath('//*[@id=\"votingmembers\"]')\n",
    "        rows = house_members.find_elements_by_tag_name(\"tr\")\n",
    "        member_info = []\n",
    "        district_info = []\n",
    "        for row in rows:\n",
    "            info_cells = row.find_elements_by_tag_name(\"td\")\n",
    "            district_cells = row.find_elements_by_tag_name(\"th\")\n",
    "            for cell in info_cells:\n",
    "                member_info.append(cell.text)\n",
    "            for cell_d in district_cells:\n",
    "                district_info.append(cell_d.text)\n",
    "        #District namves have the same tag as a table header, so let's separate the table headers from the district names\n",
    "        col_names, districts = district_info[0:8],district_info[8:]\n",
    "        col_names[-1] = 'Birthdate'\n",
    "        col_names[0:2] = col_names[0:2][::-1]\n",
    "        #There will be some blanks because the district names go by different tags, so we have to populate the missing values in our list with the district names\n",
    "        count = 0\n",
    "        for i in range(0,len(member_info)):\n",
    "            if member_info[i] == '':\n",
    "                member_info[i] = districts[count]\n",
    "                count += 1\n",
    "            if member_info[i] == 'VACANT':\n",
    "                member_info.remove('VACANT')\n",
    "                member_info.append([districts[count]])\n",
    "                for i in range(0,7):\n",
    "                    member_info.append('No House Member')\n",
    "                count += 1\n",
    "        house_array = np.reshape(np.array(member_info),(int(len(member_info)/8),8)) #Reshaping the array from one long list into an 8 by 8 matrix\n",
    "        house_df = pd.DataFrame(house_array,columns = col_names)\n",
    "        #Splitting birthdate into age and DOB\n",
    "        birthdate = house_df.Birthdate.apply(lambda x: str(x).split(' ('))\n",
    "        date_format = '%B %d, %Y'\n",
    "        birth = []\n",
    "        ages = []\n",
    "        for row in birthdate:\n",
    "            try:\n",
    "                birth.append(datetime.strptime(row[0], date_format))\n",
    "                a = re.findall(r'\\d*', row[1])\n",
    "                age = ''.join(a)\n",
    "                ages.append(int(age))\n",
    "            except:\n",
    "                birth.append(datetime.strptime('January 1, 1900', date_format))\n",
    "                ages.append(0)\n",
    "\n",
    "        house_df['Birthdate'] = pd.Series(birth)\n",
    "        house_df['Age'] = pd.Series(ages)\n",
    "        #Now let's clean the prior experience and education columns where we have new lines\n",
    "        house_df['Prior experience'] = house_df['Prior experience'].apply(lambda x: x.replace('\\n',' '))\n",
    "        house_df['Education'] = house_df['Education'].apply(lambda x: x.replace('\\n',' '))\n",
    "\n",
    "        '''#Now we're going to turn the remainder of the variables into categorical variables\n",
    "        try:\n",
    "            cat = ['Member','District','Party']\n",
    "            for col in cat:\n",
    "                house_df[col] = pd.Categorical(house_df[col])\n",
    "        except:\n",
    "            pass\n",
    "        '''\n",
    "        house_df['Assumed office'] = house_df['Assumed office'].apply(custom_date_parser)\n",
    "        driver.close()\n",
    "        return house_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ebd8c-b5fe-4ffd-bba0-d1b039ae178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional link to League of Conservation Database and comparing indicators in their census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cae20-938f-434d-9426-d74186866ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a scraper that scans the headlines for EE News Articles, \n",
    "#Conduct LDA and topic modeling, and possibly connects the municipalities mentioned in the text to census facts or their congressmen’s action on energy justice issues\n",
    "#Getting information regarding voting records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c187ae-8a9a-425b-b6de-154fc29261e2",
   "metadata": {},
   "source": [
    "<h1> Scraping EE News for the past year </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c09c0fe-33e7-499f-9174-9c6972f5c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox('/Users/ryanshen/Desktop/opt/anaconda3/bin/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c3ab51e-a5c7-44cb-b8e2-0a26b46b387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_get():\n",
    "    heading_names = []\n",
    "    \n",
    "    url_list = [] #We will be collecting a list of urls\n",
    "    author_dates = []\n",
    "    for page in range(1,50):\n",
    "        driver.get('https://www.eenews.net/publication/energywire/page/'+ str(page) +'/')\n",
    "        headings = driver.find_elements_by_tag_name(\"h4\") #first we take all the headings on the landing page\n",
    "        for heading in headings:\n",
    "            heading_names.append(heading.text)\n",
    "        urls = driver.find_elements_by_tag_name(\"a\")\n",
    "        for url in urls: #then we grab all the urls\n",
    "            if url.text == 'Read More >>': #Read More >> tagline is necessary to prevent duplication of URLs since every article on the webpage has two hyperlinks \n",
    "                url_list.append(url.get_attribute('href')) #Add the hyperlink to a list\n",
    "        author_date = driver.find_elements_by_tag_name('p')\n",
    "        for entry in author_date:\n",
    "            if entry.text[0:2] == 'BY': #All author date combos are denoted with the keyword BY\n",
    "                author_dates.append(entry.text)\n",
    "    heading_names = list(set(heading_names)) #Dropping all h4 headings that have blank values\n",
    "    author_and_dates = author_dates\n",
    "    authors = [author.split('|')[0][3:-1] for author in author_and_dates]\n",
    "    dates = [date.split('|')[1][1:-4] for date in author_and_dates]\n",
    "    \n",
    "    def str_split(string):\n",
    "        article = string.split(' ')[0]\n",
    "        date_format = '%m/%d/%Y'\n",
    "        return datetime.strptime(article, date_format)\n",
    "        \n",
    "    pub_date = list(map(str_split, dates))\n",
    "    rows = []\n",
    "    for i in range(0,len(url_list)):\n",
    "        rows.append([heading_names[i], url_list[i], authors[i], pub_date[i]])\n",
    "    articles = pd.DataFrame(rows, columns = ['headings', 'url', 'author', 'pub_date'])\n",
    "    return articles\n",
    "    #Create a function that takes the text from every URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bab7af7-146b-40cd-a148-7035e000ddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 443 444 444\n",
      "['Oil crackdown emerges as winning strategy for Colo. governor', 'Deep freeze and data concerns test Southeast power market', 'Senate climate deal: Boost or barrier for EVs?', 'EPA rule nods to public demand for EVs', '4 energy issues to watch with EPA’s power plant rule', 'People to watch at DOE, Interior, FERC', 'Miles apart: U.S. and Europe diverge on Chinese EVs', 'A blue state asks: Is carbon capture part of climate agenda?', 'Can N.M. build world’s largest coal CCS project?', 'D.C. Circuit rejects NEPA challenge to Va. pipeline expansion', 'States and clean energy: 3 issues to watch', 'FERC approves power plant rules to fight extreme weather', 'Biden admin defends solar probe, spurs industry outrage', 'White House issues ‘action plan’ to speed up energy reviews', 'Interior weighs economics of oil vs. climate in 5-year plan', 'Is Biden’s 2035 CO2 goal still achievable? What studies say', 'Can the Northeast slash carbon and keep the lights on?', 'Oil showdown: 3 ways Interior may shape industry', 'Climate law challenge: DOE staffing', 'Calif.’s last nuclear plant faces closure. Can it survive?', '‘Hubris’: LNG plant officials saw trouble days before blast', 'Former Mass. energy guru tops potential FERC nomination list', 'Biden delivers a ‘break glass’ moment for clean energy', 'What Vogtle’s stumbling finish means for U.S. nuclear energy', 'CO2 pipeline developers, foes clash over landowner lists']\n"
     ]
    }
   ],
   "source": [
    "ee_news = article_get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "273ca512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headings            object\n",
      "url                 object\n",
      "author              object\n",
      "pub_date    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ee_news.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72f1c8db-e929-41b8-8a31-182ea20bdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "        paragraph = ''\n",
    "        driver.get(url)\n",
    "        sent_elem = driver.find_elements_by_tag_name('p')#getting the sentence elements off the page\n",
    "        sent_elem = sent_elem[1::]\n",
    "        for sentences in sent_elem:\n",
    "            paragraph += sentences.text\n",
    "        return paragraph\n",
    "ee_news['text'] = ee_news['url'].apply(lambda x: get_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8e03518-1828-4e41-b2fc-571f028f0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we want to convert all cases to lowercase and remove punctuation prior to tokenization\n",
    "ee_news.text = ee_news['text'].map(lambda x: re.sub('[,\\.!?]', '', x)) \n",
    "ee_news.text = ee_news['text'].map(lambda x: x.lower())\n",
    "\n",
    "driver.close() #We've scraped all the text we've needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b02150ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_news.to_pickle('ee_news.pkl') #Converting to pickle so we don't have to scrape again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fff2ec-3f36-401a-b30b-5fdf8261b6c2",
   "metadata": {},
   "source": [
    "<h1> Performing Topic Modeling on EE News Text </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b822d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If reading this from pickle, run this cell\n",
    "ee_news = pd.read_pickle('/Users/ryanshen/Desktop/Projects/House_Voting/House-Voting/ee_news.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16850be8-2757-41d2-bca9-b9b875765fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_news.head()\n",
    "print(len(ee_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to run a generator if trying to get author counts over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72559ea-ea6d-4e14-9c5d-2a52fdb27814",
   "metadata": {},
   "source": [
    "<h4> We're now going to prime our dataframe for text analysis using <a href = https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0>LDA </a> </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2faa5-b271-42ea-84ec-b3e79d69d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea: Create heading clusters to see what the most popular topics are. Google topic modeling\n",
    "stop_words = stopwords.words('english') #First we load the english stopwords\n",
    "stop_words.extend(['from','by', 'subject', 'politico', 'photos', '/', 'said', 'would','could']) #Here's where we have the option to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63a042-9655-4de2-aba4-845fb5b7f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data = ee_news.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64298790",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_news['tokenized_text'] = data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0126e2-8d1e-4815-acb5-fc36d43a6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)# Create Corpus\n",
    "texts = data_words# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfa02a-35e9-4544-95b5-96cd71359aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10# Build LDA model. We might need to experiment to see what the ideal number of topics is\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a3cb62",
   "metadata": {},
   "source": [
    "<h4>Now we're going to determine the optimal number of topics by maximizing topic coherence.</h4>\n",
    "\n",
    "<p>Here's a <a href = 'https://radimrehurek.com/gensim/models/coherencemodel.html'> reference </a> to the gensim api, and here's a <a href = https://tedboy.github.io/nlps/_modules/gensim/models/coherencemodel.html> link </a> to the source code. <a href = https://towardsdatascience.com/understanding-topic-coherence-measures-4aa41339634c>Here's</a> a descriptive explanation of how topic modeling works.</p>\n",
    "\n",
    "<p>If you're not familiar with how topic modeling works, after confirming the number of topics, we examine our tokens to see if any of them have a high probability of being identified as a group across the entire corpus. (While topic modeling rarely examines relationships between more than two words, it is possible for three or more words to be identified as a relevant group.)\n",
    "\n",
    "The probability of two or more words being associated is also known as a confirmation measure. \n",
    "    \n",
    "After taking all the confirmation measures between each of the most important words in the topic, the confirmation measures are aggregated. The aggregate confirmation measure is the coherence score for the topic.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8741ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = range(2,30) #Trying from 2 to 30 clusters\n",
    "coherence_scores = []\n",
    "\n",
    "for n in num_topics:\n",
    "    lda_model_test = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=n)\n",
    "    cm = CoherenceModel(model=lda_model_test, corpus=corpus, coherence='u_mass') #u_mass is the fastest of the options to place within the coherence argument\n",
    "    coherence = cm.get_coherence()  # get coherence value\n",
    "    coherence_scores.append(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2,30), coherence_scores, marker='o')\n",
    "plt.axvline(x=2+coherence_scores.index(np.max(coherence_scores)), color='red', linestyle='--')\n",
    "plt.title('Coherence Scores for LDA')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Scores')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = coherence_scores.index(np.max(coherence_scores)) + 2\n",
    "print('The ideal number of topics is', num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d71700-9740-49df-8434-afdeca0d1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc594e7-918b-48e5-8f9e-d4b19235bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#We want to save the path as an absolute path when we initiate deserialization to lower the odds that the program will raise an error if the path name is too long/gets corrupted\n",
    "LDAvis_data_filepath = Path('energy_news_scraper').absolute()\n",
    "#LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "os.makedirs(os.path.dirname(LDAvis_data_filepath), exist_ok=True)\n",
    "print(type(str(LDAvis_data_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82824fe-0715-4815-8f4c-44513f756569",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tokens = id2word.token2id #Presents a dictinoary of the tokens with their token_ids\n",
    "token_freq = id2word.dfs #gets the frequency of different tokens within total documents, but this only seems to measure how many articles have the token rather than how many times these tokens are referenced\n",
    "\n",
    "merged = {}\n",
    "for key, count in dict_tokens.items():\n",
    "    merged.update({key:token_freq[count]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cad783-fe1e-4be7-888d-c2442963dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted(merged.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c070cf7-0044-43e9-b7fa-b1e2ba5c855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1564d54-42b7-4f35-9f5c-e9213fa4ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)# Print the Keyword in the 3 topics\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word) #--> Transforms and prepares data for a LDA transformation\n",
    "    with open(str(LDAvis_data_filepath), 'wb') as f: #We serialize our data to improve performance and data integrity\n",
    "        pickle.dump(LDAvis_prepared, f)# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375668c",
   "metadata": {},
   "source": [
    "Unsurprisingly, each article has a high chance of including words like power, energy, gas, and broad terms related to American, domestic energy policy. More exclusive words can be found the further you reduce lambda. Based on what I've found by setting lambda = 0.2 and 0.08, the topic numbers can be grouped as follows:\n",
    "1. Riskier Renewable Projects that include Offshore Wind and Hydrogen \n",
    "2. Solar Power Generation\n",
    "3. State Energy Policy concerning EVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bef8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train an ensemble LDA model to see if there's better performance\n",
    "#Source: https://radimrehurek.com/gensim/models/ensemblelda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify topic areas of prime interest and develop a time series of certain topic areas\n",
    "#Create a lexical dispersion plot\n",
    "#Source: https://github.com/katreparitosh/Discourse-Analytics-of-Political-Speech-Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn about the collections API https://docs.python.org/3/library/collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see if we can generate a time series of how often hydrogen is mentinoed in ee_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_count(*argv):\n",
    "    df_all_keywords = pd.DataFrame(ee_news.pub_date.unique(), columns = ['pub_date'])\n",
    "    def col_counter(list_of_words, search_word):\n",
    "        counter = Counter(list_of_words)\n",
    "        return counter[search_word]\n",
    "    \n",
    "    for keyword in argv:\n",
    "        ee_news[keyword] = [col_counter(ee_news['tokenized_text'][i], keyword) \n",
    "                               for i in range(len(ee_news['tokenized_text']))]\n",
    "        filter_df = ee_news[['pub_date', keyword]].groupby('pub_date').sum().reset_index()\n",
    "        df_all_keywords[keyword] = filter_df[keyword]\n",
    "        \n",
    "    \n",
    "    def unlimited_stripplot(dataframe, *column_names, **kwargs):\n",
    "        # Create a copy of the DataFrame with the selected columns\n",
    "        selected_data = dataframe[list(column_names)]\n",
    "\n",
    "        # Reshape the DataFrame for plotting\n",
    "        melted_df = selected_data.melt(var_name='Key Words', value_name='Frequency')\n",
    "\n",
    "        # Create a strip plot using seaborn\n",
    "        sns.stripplot(x='Frequency', y='Key Words', data=melted_df, jitter=True, **kwargs)\n",
    "\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Key Words')\n",
    "        plt.title('Articles that mention Topics')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    unlimited_stripplot(ee_news, *argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_count('solar', 'hydrogen', 'wind') #A latency plot will probably be better here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ae292",
   "metadata": {},
   "source": [
    "Here we're creating a lexical dispersion plot, which aims to visualize how often words appear from the begnning of the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(itertools.chain(*ee_news.tokenized_text))\n",
    "all_articles_Text = nltk.Text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_Text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_Text.similar('energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13cdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_Text.similar('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_Text.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * all_articles_Text.count('energy') / len(all_articles_Text) #Energy takes up 1.2% of total words used in the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting lexical richness\n",
    "len(set(all_articles_Text))/len(all_articles_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef81114",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''target_words = ['money','energy']\n",
    "\n",
    "all_articles_Text.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n",
    "#Why is this lexical dispersion plot not graphing any results?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77148290",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_Text.collocations(num = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_articles_Text, window_size=7)\n",
    "finder.nbest(bigram_measures.pmi, 10)\n",
    "finder.apply_freq_filter(10)\n",
    "results=finder.nbest(bigram_measures.pmi,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf56526",
   "metadata": {},
   "outputs": [],
   "source": [
    "results #We're getting quite a few names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f22d1c-604d-44c9-aeb2-729de699ea60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Option, turn every article into its own class\n",
    "#Inspiration: https://www.geeksforgeeks.org/self-in-python-class/#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
